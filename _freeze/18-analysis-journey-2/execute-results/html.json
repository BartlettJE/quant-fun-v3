{
  "hash": "937723f5cc5172bbccbbc5e47d69533f",
  "result": {
    "markdown": "# Analysis Journey 2: Simple Linear Regression {#journey-02-wrangling}\n\nWelcome to the second data analysis journey. We have designed these chapters as a bridge between the structured learning in the core chapters and your assessments. We present you with a new data set, show you what the end product should look like, and see if you can apply your data wrangling, visualisation, and/or analysis skills to get there. \n\nAs you gain independence, this is the crucial skill. Data analysis is all about seeing the data you have available to you and identifying what the end product needs to be to apply your visualisation and analysis techniques. You can then mentally (or physically) create a checklist of tasks to work backgrounds to get there. There might be a lot of trial and error as you try one thing, it does not quite work, so you go back and try something else. If you get stuck though, we have a range of hints and task lists you can unhide, then the solution to check your attempts against. \n\nIn this second data analysis journey, we focus on inferential statistics after some data wrangling to apply all the skills you developed from Chapter 1 to Chapter 9. \n\n## Task preparation\n\n### Introduction to the data set \n\nFor this task, we are using open data from @binfet_importance_2022, where the authors used the data set to write a separate article on repurposing it for statistics education [@evans_repurposing_2023], inspiring us to use it in the chapter here. The abstract of their article is: \n\n> Researchers have claimed that canine-assisted interventions (CAIs) contribute significantly to bolstering participants' wellbeing, yet the mechanisms within interactions have received little empirical attention. The aim of this study was to assess the impact of client–canine contact on wellbeing outcomes in a sample of 284 undergraduate college students (77% female; 21% male, 2% non-binary). Participants self-selected to participate and were randomly assigned to one of two canine interaction treatment conditions (touch or no touch) or to a handler-only condition with no therapy dog present. To assess self-reports of wellbeing, measures of flourishing, positive and negative affect, social connectedness, happiness, integration into the campus community, stress, homesickness, and loneliness were administered. Exploratory analyses were conducted to assess whether these wellbeing measures could be considered as measuring a unidimensional construct. This included both reliability analysis and exploratory factor analysis. Based on the results of these analyses we created a composite measure using participant scores on a latent factor. We then conducted the tests of the four hypotheses using these factor scores. Results indicate that participants across all conditions experienced enhanced wellbeing on several measures; however, only those in the direct contact condition reported significant improvements on all measures of wellbeing. Additionally, direct interactions with therapy dogs through touch elicited greater wellbeing benefits than did no touch/indirect interactions or interactions with only a dog handler. Similarly, analyses using scores on the wellbeing factor indicated significant improvement in wellbeing across all conditions (handler-only, *d*=0.18, *p*=0.041; indirect, *d*=0.38, *p*<0.001; direct, *d*=0.78, *p*<0.001), with more benefit when a dog was present (*d*=0.20, *p*<0.001), and the most benefit coming from physical contact with the dog (*d*=0.13, *p*=0.002). The findings hold implications for post-secondary wellbeing programs as well as the organization and delivery of CAIs.\n\nIn summary, they were interested in the effect of therapy dogs on well-being in undergraduate students. Participants were randomly allocated to one of three groups: \n\n1. Canine interaction touching the dogs. \n\n2. Canine interaction not touching the dogs. \n\n3. Handler-only with no dogs present. \n\nThey measured 9 outcomes before and after the intervention including social connectedness, stress, and loneliness. For this journey chapter, we will focus on a constrained set of variables and analyses so it does not take forever, but the process would apply to all the outcomes. The authors posed three hypotheses which we will test after some data wrangling:\n\n1. All treatment groups would have significantly higher measures of well-being and lower measures of ill-being after treatment. \n\n2. The treatment groups that interact with dogs would have significantly higher measures of well-being and lower measures of ill-being compared to the handler-only treatment. \n\n3. Direct contact with a therapy dog would yield greater benefits than indirect contact treatment.\n\n### Organising your files and project for the task\n\nBefore we can get started, you need to organise your files and project for the task, so your working directory is in order.\n\n1. In your folder for research methods and the book `ResearchMethods1_2/Quant_Fundamentals`, create a new folder for the data analysis journey called `Journey_02_regression`. Within `Journey_02_regression`, create two new folders called `data` and `figures`.\n\n2. Create an R Project for `Journey_02_regression` as an existing directory for your chapter folder. This should now be your working directory.\n\n3. Create a new R Markdown document and give it a sensible title describing the chapter, such as `Analysis Journey 2 - Simple Linear Regression`. Delete everything below line 10 so you have a blank file to work with and save the file in your `Journey_02_regression` folder. \n\n4. We are working with a new data set, so please save the following data file: [Evans_2023_raw.csv](data/Evans_2023_raw.csv). Right click the link and select \"save link as\", or clicking the link will save the files to your Downloads. Make sure that you save the file as \".csv\". Save or copy the file to your `data/` folder within `Journey_02_regression`.\n\nYou are now ready to start working on the task! \n\n## Overview\n\n### Load <pkg>tidyverse</pkg> and read the data files\n\nBefore we explore what wrangling we need to do, complete the following task list and check the solution if you are stuck. \n\n::: {.callout-tip}\n#### Try this\nComplete the following steps:\n\n1. Load the following packages:\n\n    - <pkg>tidyverse</pkg>\n    \n    - <pkg>effectsize</pkg>\n    \n    - <pkg>performance</pkg>\n\n2. Read the data file `data/Evans_2023_raw.csv` to the object name `evans_data`.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n#### Show me the solution\nYou should have the following in a code chunk: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load the relevant packages\nlibrary(effectsize)\nlibrary(performance)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'performance' was built under R version 4.3.3\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\n# Read the Evans_2023_raw.csv file \nevans_data <- read_csv(\"data/Evans_2023_raw.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 284 Columns: 88\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): GroupAssignment\ndbl (87): RID, Age_Yrs, Year_of_Study, Live_Pets, Consumer_BARK, S1_1, L1_1,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n:::\n\n### Explore `evans_data`\n\nIn `evans_data`, we have the participant ID (`RID`), several demographic variables, and pre and post items for stress, loneliness, and social connectedness. There are 88 variables which would take up loads of space, so we are just showing a preview of the first 20 here. If you use `glimpse()`, you will see all 88. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\nRows: 284\nColumns: 20\n$ RID             <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ GroupAssignment <chr> \"Control\", \"Direct\", \"Indirect\", \"Control\", \"Direct\", …\n$ Age_Yrs         <dbl> 21, 19, 18, 18, 19, 20, 26, 17, 21, 22, 19, 20, 19, 19…\n$ Year_of_Study   <dbl> 3, 1, 1, 1, 1, 2, 2, 1, 3, 4, 2, 2, 2, 2, 3, 1, 1, 1, …\n$ Live_Pets       <dbl> 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, NA, 2, 2, 1,…\n$ Consumer_BARK   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, …\n$ S1_1            <dbl> 2, 2, 4, 2, 3, 4, 4, 3, 2, 2, 3, 2, 3, 4, 3, 2, 4, 2, …\n$ L1_1            <dbl> 3, 3, 3, 4, 2, 4, 3, 2, 3, 4, 3, 3, 4, 4, 4, 2, 3, 4, …\n$ L1_2            <dbl> 3, 2, 3, 2, 3, 3, 2, 3, 4, 1, 3, 3, 2, 3, 1, 4, 3, 2, …\n$ L1_3            <dbl> 4, 3, 2, 2, 3, 3, 1, 3, 3, 1, 2, 2, 1, 3, 1, 3, 3, 1, …\n$ L1_4            <dbl> 3, 3, 3, 3, 3, 3, 3, 3, 4, 1, 2, 3, 2, 3, 2, 4, 3, 2, …\n$ L1_5            <dbl> 2, 4, 3, 4, 4, 3, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, …\n$ L1_6            <dbl> 3, 3, 4, 4, 3, 2, 3, 3, 3, 4, 3, 3, 4, 3, 4, 2, 2, 4, …\n$ L1_7            <dbl> 1, 2, 2, 1, 2, 2, 4, 2, 2, 1, 3, 2, 3, 2, 2, 2, 3, 3, …\n$ L1_8            <dbl> 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 4, 3, 2, …\n$ L1_9            <dbl> 3, 4, 3, 3, 3, 3, 3, 3, 2, 4, 3, 3, 4, 4, 4, 3, 2, 3, …\n$ L1_10           <dbl> 4, 3, 3, 4, 2, 2, 3, 3, 3, 4, 4, 3, 4, 4, 4, 2, 2, 3, …\n$ L1_11           <dbl> 3, 2, 2, 2, 4, 3, 4, 2, 2, 1, 3, 2, 2, 2, 2, 4, 3, 2, …\n$ L1_12           <dbl> 1, 2, 2, 1, 4, 3, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 3, 1, …\n$ L1_13           <dbl> 3, 1, 2, 2, 4, 3, 3, 3, 4, 1, 3, 4, 2, 2, 2, 4, 3, 2, …\n```\n:::\n:::\n\n\nThe columns (variables) we have in the data set are:\n\n| Variable       |       Type                       |           Description          |\n|:--------------:|:---------------------------------|:-------------------------------|\n| RID | double| Participant ID number. |\n| GroupAssignment | character| Randomly allocated study group: Control, Indirect, Direct. |\n| Age_Yrs | double| Age in years. |\n| Year_of_Study | double| Participant's year in college: First (1), Second (2), Third (3), Fourth (4), Fifth or more (5). |\n| Live_Pets | double| Does the participant have a pet back at home: Pet back home (1), no pet back home (2). |\n| Consumer_BARK | double| Is the participant a low (1), medium (2), or high (3) consumer of the BARK program - the therapy dog service. |\n| S1_1 | double| Stress scale pre-test, 1 item, 1 (not at all stressed) to 5 (very stressed). |\n| L1_1 to L1_20 | double| Loneliness scale pre-test, 20 items, 1 (never) to 4 (often). |\n| SC1_1 to SC1_20 | double| Social connectedness scale pre-test, 20 items, 1 (strongly disagree) to 6 (strongly agree). |\n| S2_1 | double| Stress scale post-test, 1 item. |\n| L2_1 to L2_20 | double| Loneliness scale post-test, 20 items. |\n| SC2_1 to SC2_20 | double| Social connectedness scale post-test, 20 items, 1 (strongly disagree) to 6 (strongly agree).|\n\n::: {.callout-tip}\n#### Try this\nNow we have introduced the data set, explore them using different methods we introduced. For example, opening the data object as a tab to scroll around, explore with `glimpse()`, or even try plotting some of the individual variables to see what they look like.\n:::\n\n## Wrangling \n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nWe are going to show you a preview of the starting data set and the end product we are aiming for. For the raw data, we have limited this to the first 20 rows again just so it does not take up the whole page, but if you use `glimpse()` you will see all 88 variables.\n\n::: {.panel-tabset}\n#### Raw data\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\nRows: 284\nColumns: 20\n$ RID             <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ GroupAssignment <chr> \"Control\", \"Direct\", \"Indirect\", \"Control\", \"Direct\", …\n$ Age_Yrs         <dbl> 21, 19, 18, 18, 19, 20, 26, 17, 21, 22, 19, 20, 19, 19…\n$ Year_of_Study   <dbl> 3, 1, 1, 1, 1, 2, 2, 1, 3, 4, 2, 2, 2, 2, 3, 1, 1, 1, …\n$ Live_Pets       <dbl> 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, NA, 2, 2, 1,…\n$ Consumer_BARK   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, …\n$ S1_1            <dbl> 2, 2, 4, 2, 3, 4, 4, 3, 2, 2, 3, 2, 3, 4, 3, 2, 4, 2, …\n$ L1_1            <dbl> 3, 3, 3, 4, 2, 4, 3, 2, 3, 4, 3, 3, 4, 4, 4, 2, 3, 4, …\n$ L1_2            <dbl> 3, 2, 3, 2, 3, 3, 2, 3, 4, 1, 3, 3, 2, 3, 1, 4, 3, 2, …\n$ L1_3            <dbl> 4, 3, 2, 2, 3, 3, 1, 3, 3, 1, 2, 2, 1, 3, 1, 3, 3, 1, …\n$ L1_4            <dbl> 3, 3, 3, 3, 3, 3, 3, 3, 4, 1, 2, 3, 2, 3, 2, 4, 3, 2, …\n$ L1_5            <dbl> 2, 4, 3, 4, 4, 3, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, …\n$ L1_6            <dbl> 3, 3, 4, 4, 3, 2, 3, 3, 3, 4, 3, 3, 4, 3, 4, 2, 2, 4, …\n$ L1_7            <dbl> 1, 2, 2, 1, 2, 2, 4, 2, 2, 1, 3, 2, 3, 2, 2, 2, 3, 3, …\n$ L1_8            <dbl> 2, 2, 3, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 4, 3, 2, …\n$ L1_9            <dbl> 3, 4, 3, 3, 3, 3, 3, 3, 2, 4, 3, 3, 4, 4, 4, 3, 2, 3, …\n$ L1_10           <dbl> 4, 3, 3, 4, 2, 2, 3, 3, 3, 4, 4, 3, 4, 4, 4, 2, 2, 3, …\n$ L1_11           <dbl> 3, 2, 2, 2, 4, 3, 4, 2, 2, 1, 3, 2, 2, 2, 2, 4, 3, 2, …\n$ L1_12           <dbl> 1, 2, 2, 1, 4, 3, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 3, 1, …\n$ L1_13           <dbl> 3, 1, 2, 2, 4, 3, 3, 3, 4, 1, 3, 4, 2, 2, 2, 4, 3, 2, …\n```\n:::\n:::\n\n\n#### Wrangled data\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\nRows: 284\nColumns: 12\n$ RID             <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ GroupAssignment <chr> \"Control\", \"Direct\", \"Indirect\", \"Control\", \"Direct\", …\n$ Age_Yrs         <dbl> 21, 19, 18, 18, 19, 20, 26, 17, 21, 22, 19, 20, 19, 19…\n$ Year_of_Study   <dbl> 3, 1, 1, 1, 1, 2, 2, 1, 3, 4, 2, 2, 2, 2, 3, 1, 1, 1, …\n$ Live_Pets       <chr> \"Does not have a pet back home\", \"Does not have a pet …\n$ Consumer_BARK   <chr> \"Low\", \"Low\", \"Low\", \"Low\", \"Low\", \"Low\", \"Low\", \"Low\"…\n$ stress_pre      <dbl> 2, 2, 4, 2, 3, 4, 4, 3, 2, 2, 3, 2, 3, 4, 3, 2, 4, 2, …\n$ stress_post     <dbl> 2, 1, 3, 2, 4, 4, 3, 2, 2, 1, 2, 2, 1, 2, 4, 2, 2, 1, …\n$ lonely_pre      <dbl> 2.25, 1.90, 2.25, 1.75, 2.85, 2.70, 2.40, 2.25, 2.55, …\n$ lonely_post     <dbl> 1.70, 1.60, 2.25, 2.05, 2.70, 2.40, 2.25, 2.00, 2.55, …\n$ social_pre      <dbl> 3.90, 5.15, 4.10, 4.65, 3.65, 4.35, 4.75, 4.60, 4.20, …\n$ social_post     <dbl> 3.800000, 5.263158, 4.150000, 5.100000, 3.600000, 4.65…\n```\n:::\n:::\n\n:::\n\n::: {.callout-tip}\n#### Try this\n\nBefore we give you a task list, try and switch between the raw data and the wrangled data. Make a list of all the differences you can see between the two data objects. \n\n1. Do the values of variables change from numbers? How might you recode them using the code book above?  \n\n2. Looking at the codebook, are some variables the same but renamed? \n\n3. Looking at the codebook, have we calculated the mean of all the items for a scale?\n\nTry and wrangle the data based on all the differences you notice to create a new object `evans_wide`. \n\nFor one hint, unless you read the original paper, there are a bunch of items that first need reverse coding you would not know about:\n\n- Loneliness pre-test: L1_1, L1_5, L1_6, L1_9, L1_10, L1_15, L1_16, L1_19, L1_20.\n\n- Loneliness post-test: L2_1, L2_5, L2_6, L2_9, L2_10, L2_15, L2_16, L2_19, L2_20.\n                          \n- Social connectedness pre-test: SC1_3, SC1_6, SC1_7, SC1_9, SC1_11, SC1_13, SC1_15, SC1_17, SC1_18, SC1_20.\n\n- Social connectedness post-test: SC2_3, SC2_6, SC2_7, SC2_9, SC2_11, SC2_13, SC2_15, SC2_17, SC2_18, SC2_20.\n\nWhen you get as far as you can, check the task list which explains all the steps we applied, but not how to do them. Then, you can check the solution for our code. \n\n:::\n\n### Task list\n\n::: {.callout-note collapse=\"true\"} \n#### Show me the task list\n\nThese are all the steps we applied to create the wrangled data object:\n\n1. Recode `Live_Pets` to the two labels outlined in the code book. \n\n2. Recode `Consumer_BARK` to the three labels outlined in the code book. \n\n3. Reverse code the loneliness and social connectedness items outlined above. Think of previous examples where we explained reverse coding for how you can do this efficiently. \n\nAs one extra piece of advice if you do not want to recode 40 variables one by one, there is a more advanced function you can use within `mutate()`. The function `across()` lets you apply a function or calculation to several columns at once. For example, if we wanted to reverse score items on a 4-point scale, it would look like the following: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmutate(across(.cols = c(column1, column2...), \n              .fns = ~ 5 - .x))\n```\n:::\n\n\nIn `.cols`, we enter all the columns we want to apply the function to. \n\nIn `.fns` after the `=`, we add the function we want to apply to all the columns we selected. The code is a little awkward as we have a tilde `~`, here the calculation we want to apply, and `.x` in place of the column name. You could summarise it as: for all the columns I select, subtract each value from 5. Once you get used to the format, `across()` is really helpful when you want to do the same thing to multiple columns. \n\n4. After reverse coding the items, calculate the subscale mean scores for loneliness and social connectedness. You must do this twice per scale, as we have the 20 items for the pre-test and 20 items for the post-test per scale. \n\n5. If you calculated the subscale mean scores individually, join them back to the `evans_clean` object you mutated. \n\n6. Select the following columns: \n\n    - `RID` to `Consumer_BARK`. \n\n    - Rename `S1_1` to `stress_pre`.\n    \n    - Rename `S2_1` to `stress_post`.\n    \n    - Select your four subscale mean score variables. \n\nRemember: If it's easier for you to complete steps with longer but accurate code, there is nothing wrong with that. You recognise ways to make your code more efficient over time. \n\n:::\n\n### Solution\n\n::: {.callout-caution collapse=\"true\"} \n#### Show me the solution\n\nThis is the code we used to create the new object `evans_wide` using the original object `evans_data`. As long as you get the same end result, the exact code is not important. In coding, there are multiple ways of getting to the same end result. Maybe you found a more efficient way to complete some of the steps compared to us. Maybe your code was a little longer. As long as it worked, that is the most important thing.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Initial cleaning step to recode pets and BARK\n# then reverse code a bunch of items \nevans_clean <- evans_data %>% \n  mutate(Live_Pets = case_match(Live_Pets,\n                                1 ~ \"Has a pet back home\",\n                                2 ~ \"Does not have a pet back home\"),\n         Consumer_BARK = case_match(Consumer_BARK,\n                                    1 ~ \"Low\",\n                                    2 ~ \"Medium\",\n                                    3 ~ \"High\"),\n         # across works with mutate to apply the same function to several columns\n         # So, take all the loneliness items to reverse code, then subtract them from 5\n         across(.cols = c(L1_1, L1_5, L1_6, L1_9, L1_10, L1_15, L1_16, L1_19, L1_20,\n                          L2_1, L2_5, L2_6, L2_9, L2_10, L2_15, L2_16, L2_19, L2_20),\n                .fns = ~ 5 - .x),\n         # take all the connectedness items to reverse code, then subtract them from 7\n         across(.cols = c(SC1_3, SC1_6, SC1_7, SC1_9, SC1_11, SC1_13, SC1_15, SC1_17, SC1_18, SC1_20, \n                          SC2_3, SC2_6, SC2_7, SC2_9, SC2_11, SC2_13, SC2_15, SC2_17, SC2_18, SC2_20),\n                .fns = ~ 7 - .x))\n\n# There are more elegant ways around this, but for each set, \n# take the 20 items, group by participant ID, and calculate the mean, ignoring missing values\nlonely_pre <- evans_clean %>% \n  pivot_longer(cols = L1_1:L1_20, \n               names_to = \"Item\", \n               values_to = \"Response\") %>% \n  group_by(RID) %>% \n  summarise(lonely_pre = mean(Response, na.rm = TRUE))\n\n# Same thing for post scores\nlonely_post <- evans_clean %>% \n  pivot_longer(cols = L2_1:L2_20, \n               names_to = \"Item\", \n               values_to = \"Response\") %>% \n  group_by(RID) %>% \n  summarise(lonely_post = mean(Response, na.rm = TRUE))\n\n# take the 20 items, group by participant ID, and calculate the mean, ignoring missing values\nsocial_pre <- evans_clean %>% \n  pivot_longer(cols = SC1_1:SC1_20, \n               names_to = \"Item\", \n               values_to = \"Response\") %>% \n  group_by(RID) %>% \n  summarise(social_pre = mean(Response, na.rm = TRUE))\n\n# Same thing for post scores\nsocial_post <- evans_clean %>% \n  pivot_longer(cols = SC2_1:SC2_20, \n               names_to = \"Item\", \n               values_to = \"Response\") %>% \n  group_by(RID) %>% \n  summarise(social_post = mean(Response, na.rm = TRUE))\n\n# join all four summary values to main data\n# select just the key variables we need\n# rename the two stress items\nevans_wide <- evans_clean %>% \n  inner_join(lonely_pre) %>% \n  inner_join(lonely_post) %>% \n  inner_join(social_pre) %>% \n  inner_join(social_post) %>% \n  select(RID:Consumer_BARK, \n         stress_pre = S1_1, \n         stress_post = S2_1, \n         lonely_pre:social_post)\n```\n:::\n\n:::\n\n## Summarising/visualising \n\nYou should now have an object called `evans_wide` containing 12 variables. If you struggled completing the wrangling steps, you can copy the code from the solution to follow along from this point. In this section, we will calculate some summary statistics and plot the data to see what we can learn. We present you with a list of questions to answer using your wrangling and visualisation skills, interspersed with the solutions to check if you are stuck. \n\n### Demographics\n\nFor demographics, we will recreate some values from Table 1 from @binfet_importance_2022.\n\n1. How many participants were in each group for `GroupAssignment`? \n\n    - <input class='webex-solveme nospaces' size='2' data-answer='[\"94\"]'/> Control\n    \n    - <input class='webex-solveme nospaces' size='2' data-answer='[\"95\"]'/> Direct\n    \n    - <input class='webex-solveme nospaces' size='2' data-answer='[\"95\"]'/> Indirect\n\n::: {.callout-caution collapse=\"true\"} \n#### Show me the solution\n\nThis nicely reproduces their values. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nevans_wide %>% \n  count(GroupAssignment)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|GroupAssignment |  n|\n|:---------------|--:|\n|Control         | 94|\n|Direct          | 95|\n|Indirect        | 95|\n\n</div>\n:::\n:::\n\n:::\n\n2. To 2 decimals, what was the mean and standard deviation age per group? \n\n    - Control: *M* = <input class='webex-solveme nospaces' size='5' data-answer='[\"19.95\"]'/>, *SD* = <input class='webex-solveme nospaces' size='4' data-answer='[\"2.89\"]'/>. \n    \n    - Direct: *M* = <input class='webex-solveme nospaces' size='5' data-answer='[\"19.77\"]'/>, *SD* = <input class='webex-solveme nospaces' size='4' data-answer='[\"1.94\"]'/>.\n    \n    - Indirect: *M* = <input class='webex-solveme nospaces' size='5' data-answer='[\"19.95\"]'/>, *SD* = <input class='webex-solveme nospaces' size='4' data-answer='[\"2.23\"]'/>.\n\n::: {.callout-caution collapse=\"true\"} \n#### Show me the solution\n\nWeirdly, this reproduces the standard deviations from Table 1, but not the means. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nevans_wide %>% \n  group_by(GroupAssignment) %>% \n  summarise(mean_age = round(mean(Age_Yrs), 2),\n            sd_age = round(sd(Age_Yrs), 2))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|GroupAssignment | mean_age| sd_age|\n|:---------------|--------:|------:|\n|Control         |    19.95|   2.89|\n|Direct          |    19.77|   1.94|\n|Indirect        |    19.95|   2.23|\n\n</div>\n:::\n:::\n\n:::\n\n3. How many participants in each group have a pet at home? \n\n    - <input class='webex-solveme nospaces' size='2' data-answer='[\"21\"]'/> Control\n    \n    - <input class='webex-solveme nospaces' size='2' data-answer='[\"30\"]'/> Direct\n    \n    - <input class='webex-solveme nospaces' size='2' data-answer='[\"34\"]'/> Indirect\n\n::: {.callout-caution collapse=\"true\"} \n#### Show me the solution\n\nThis nicely reproduces their values. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nevans_wide %>% \n  count(GroupAssignment, Live_Pets)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|GroupAssignment |Live_Pets                     |  n|\n|:---------------|:-----------------------------|--:|\n|Control         |Does not have a pet back home | 72|\n|Control         |Has a pet back home           | 21|\n|Control         |NA                            |  1|\n|Direct          |Does not have a pet back home | 63|\n|Direct          |Has a pet back home           | 30|\n|Direct          |NA                            |  2|\n|Indirect        |Does not have a pet back home | 60|\n|Indirect        |Has a pet back home           | 34|\n|Indirect        |NA                            |  1|\n\n</div>\n:::\n:::\n\n:::\n\n4. This is not part of their article, but one interesting question might be how many people have a pet at home (`Live_Pets`) against how frequently they use the BARK program (`Consumer_BARK`). Try and recreate the following bar plot to visualise this as close as possible. We have intentionally used some features we have not covered in the data visualisation materials to get you problem solving. For one hint though, we used option D for the viridis colour scheme. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](18-analysis-journey-2_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-caution collapse=\"true\"} \n#### Show me the solution\n\nThe new features we hoped you found independently were: \n\n- Setting the factor order to show `Consumer_BARK` as low, medium, then high. \n\n- Set `position = \"dodge\"` to avoid the stacked bar chart. \n\n- Edited the legend title by using the `name` argument in the `scale_fill` layer. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nevans_wide %>% \n  drop_na(Live_Pets, Consumer_BARK) %>% \n  mutate(Consumer_BARK = factor(Consumer_BARK, \n                                levels = c(\"Low\", \"Medium\", \"High\"))) %>% \n  ggplot(aes(x = Live_Pets, fill = Consumer_BARK)) + \n  geom_bar(position = \"dodge\") + \n  labs(x = \"Pets in Home\", \n       y = \"Frequency\") + \n  scale_fill_viridis_d(option = \"D\", \n                       name = \"BARK Program User\") + \n  theme_classic()\n```\n:::\n\n:::\n\n### Wellbeing measures\n\nFor wellbeing and ill-being measures, we will recreate some values from Table 2 from @binfet_importance_2022.\n\n## Analysing \n\n### Hypothesis 1\n\n### Hypothesis 2\n\n### Hypothesis 3\n\n## Conclusion\n",
    "supporting": [
      "18-analysis-journey-2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}