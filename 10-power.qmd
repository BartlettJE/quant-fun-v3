# Statistical Power and Effect Sizes

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(pwr)
library(tidyverse)
library(cowplot)
```

Up until now we have mainly spent time on data-wrangling, understanding probability, visualising our data, and more recently, running inferential tests, i.e. t-tests. In the lectures, however, you have also started to learn about additional aspects of inferential testing and trying to reduce certain types of error in your analyses. It is this balance of minimising error in our inferential statistics that we will focus on today. 

This chapter is adapted from @bartlett_power_2022 and we recommend referring to this article for more information on the concepts behind statistical power. However, this article focuses on the statistics software jamovi, so this chapter focuses on power analysis using the <pkg>pwr</pkg> package.

**Chapter Intended Learning Outcomes (ILOs)**

By the end of this chapter, you will be able to: 

- ILO1

## Chapter preparation

### Organising your files and project for the chapter

In contrast to previous chapters, there will be no data wrangling in this chapter, so we do not need to worry about downloading files. We will still be working around some key studies, but we will introduce them as needed. However, you will still be working in an R Markdown document, so you need to make sure your working directory is in order.

1. In your folder for research methods and the book `ResearchMethods1_2/Quant_Fundamentals`, create a new folder called `Chapter_10_power`.

2. Create an R Project for `Chapter_10_power` as an existing directory for your chapter folder. This should now be your working directory.

3. Create a new R Markdown document and give it a sensible title describing the chapter, such as `10 Statistical Power and Effect Sizes`. Delete everything below line 10 so you have a blank file to work with and save the file in your `Chapter_10_power` folder. 

4. In a code chunk, load the <pkg>pwr</pkg> and <pkg>tidyverse</pkg> packages. If you need to install any packages, revisit [Chapter 1](#install-tidy) if you need a refresher, but remember not to install packages on the university computers / online server. 

You are now ready to start working on the chapter! 

## NHST and statistical power recap

Given there is no data wrangling for this chapter and the functions for power analysis are pretty straight forward, we have a little space to recap the key concepts behind power analysis. Almost all the work here comes into thinking and justifying your decisions, rather than spending lots of time on wrangling and analysis.

The branch of statistics we are using here is Null Hypothesis Significance Testing (NHST). There are two types of `r glossary("hypothesis", display = "hypotheses")` and what you are trying to establish is the `r glossary("probability")` of rejecting the null hypothesis. Those two hypotheses are:

- The **null hypothesis** which states that there is no difference ($H_0: \mu_1 = \mu_2$) or no relationship ($H_0: r = 0$). 
- The **alternative hypothesis** which states that there is a difference ($H_1: \mu_1 \ne \mu_2$) or there is a relationship ($H_1: r \ne 0$).

NHST is designed to control error rates associated with two main decisions around these hypotheses:

- **Type I error** - or `r glossary("false positive")`, is the probability of rejecting the null hypothesis when it should not be rejected (otherwise called `r glossary("alpha")` or $\alpha$). In other words, you conclude that there is a real "effect" when in fact there is no effect. The field standard rate of acceptable false positives is $\alpha = .05$, meaning that we would accept 1 in 20 studies may be a false positive.

- **Type II error** - or `r glossary("false negative")`, is the probability of retaining the null hypothesis when it should be rejected (otherwise called `r glossary("beta")` or $\beta$). In other words, you conclude that there was no real "effect" when in fact there was one. There is less tradition around this, but the most common rule of thumb you will come across is $\beta = .20$, meaning that we would accept 1 in 5 studies may be a false negative. 

Statistical power is the opposite of beta and represents the long-run probability of correctly rejecting the null hypothesis when there is a real effect to detect. In other words, how likely are you to detect an effect that is really there? You calculate `r glossary("power", display = "Power")` as $1-\beta$, meaning that if the field standard for beta is $\beta = .20$, then the field standard for power is $1 - .20 = .80$.

In addition to alpha and beta/power, there are two other key concepts (there are more depending on the test, but we will add them in when we need them):

- **Effect size** - A number that expresses the magnitude of the phenomenon relevant to your research question. In Chapters 8 and 9, we introduced you to different standardised effect sizes such as Pearson's *r* and Cohen's d. 

- **Sample size** - The number of observations (usually participants, but it might be animals or stimuli depending on your topic) in your study. 

Critically, there is a relationship between these four concepts, where if you know three, you can calculate the **fourth** in a process called **power analysis**. The two most useful types of power analysis are:

1. **A priori** power analysis: How many participants do I need, for a given alpha, beta/power, and smallest effect size of interest? This is most useful in the design stage to help you plan how many participants you need to design an informative study. 

2. **Sensitivity** power analysis: What effect size can I detect, for a given alpha, beta/power, and sample size? This is most useful after you finish collecting data or when you are using secondary data as the sample size is not longer under your control and it helps put your findings in context. 

You may now be thinking though, if there is a relationship between all four concepts, could we calculate power for a given alpha, sample size, and effect size? It is a tempting idea and you might see some articles report it, but unfortunately is is misleading and tells you nothing more than the *p*-value does. The short version is we are typically using a sample to learn something about a population, so there is uncertainty in the estimate. Using the observed effect size in your study assumes this is the true population effect, which is rarely a good assumption. If you are interested, @lakens_sample_2022 is a great resource for sample size justification in general, but also explains why post-hoc power is not a good idea.

After the brief recap, we will now focus on *a priori* and sensitivity power analyses applied to different statistical models. 

## Power analysis for t-tests / categorical predictors

### Introduction to the study

In this section, imagine we are designing a study to build on @irving_correcting_2022 who tested an intervention to correct statistical misinformation. Participants read an article about a new fictional study where one passage falsely concludes watching TV causes cognitive decline. In the correction group, participants receive an extra passage where the fictional researcher explains they only reported a correlation, not a causal relationship. In the no-correction group, the extra passage just explains the fictional researcher was not available to comment. Irving et al. then tested participants’ comprehension of the story and coded their answers for mistaken causal inferences. They expected participants in the correction group to make fewer causal inferences than those in the no-correction group, and found evidence supporting this prediction with an effect size equivalent to Cohen’s d = 0.64, 95% CI = [0.28, 0.99]. Inspired by their study, we want to design an experiment to correct another type of misinformation in articles.

Irving et al. (2022) themselves provide an excellent example of explaining and justifying the rationale behind their power analysis, so we will walk through the decision making process and how it changes the outputs. For our smallest effect size of interest, our starting point is the estimate of d = 0.64. However, it is worth consulting other sources to calibrate our understanding of effects in the area, such as Irving et al. citing a meta-analysis. For debunking, the average effect across 30 studies was d = 1.14, 95% CI = [0.68, 1.61], so we could use the lower bound of the confidence interval, but this may still represent an overestimate. Irving et al. used the smallest effect (d = 0.54) from the studies most similar to their design which was included in the meta-analysis. As a value slightly smaller than the other estimates, we will also use this as the smallest effect of interest for our study. 

Now we have settled on our smallest effect size of interest, we will use d = 0.54 in the following demonstrations. We start with a priori and sensitivity power analysis for two independent samples, exploring how the outputs change as we alter inputs like alpha, power, and the number of tails in the test. For each demonstration, we explain how you can transparently report the power analysis to your reader. We then repeat the demonstrations for two dependent samples to show how you require fewer participants when the same participants complete multiple conditions instead of being allocated to separate groups.   

### A priori power analysis

For an independent samples t-test (we will cover regression shortly, but one of the downsides is the power analysis function uses a different effect size which can be awkward), there is the function `pwr.t.test()`. We can enter the following arguments: 

- `n`: The number of observations. 

- `d`: The effect size as Cohen's d. 

- `sig.level`: The alpha level. 

- `power`: The power value as 1-$\beta$. 

- `type`: Whether you want power for a one-, paired-, or independant-samples t-test.  

- `alternative`: Whether you have a one- or two-sided hypothesis. 

Remember, power analysis works by leaving one argument blank, so for calculating the sample size, we leave `n` blank or enter `NULL` as the argument. To recreate the power analysis from @irving_correcting_2022, we enter d = 0.54, sig.level = .05, power = .90, type = two.sample and alternative = two-sided. 

```{r}
pwr.t.test(n = NULL, 
           d = 0.54, 
           sig.level = .05, 
           power = .90, 
           type = "two.sample", 
           alternative = "two.sided")
```

As the note warns us, for an independent-samples t-test, n represents the number of observations per group, so we need **74** per group (we round up as we cannot have .04 of a person) or N = **148**.

If you wanted to use these values in inline code, you can save the power analysis object and pick out values to work with. 

```{r}
irving_samplesize <- pwr.t.test(n = NULL, 
                           d = 0.54, 
                           sig.level = .05, 
                           power = .90, 
                           type = "two.sample", 
                           alternative = "two.sided") %>% 
  pluck("n") %>% 
  ceiling()
```

In this code, we save the power analysis function to an object, use the `pluck()` function to pick out as a specific component (the argument name must be spelt exactly), and use the `ceiling()` function to round up. This helps to avoid manually calculating the values as you can use the object. 

```{r}
# sample size per group
irving_samplesize

# total sample size
irving_samplesize * 2
```

The power analysis function is pretty straight forward to work with, it is the thinking and decision making that does into selecting the value for each argument that is the hardest part here. It is ultimately a subjective decision you must be able to justify in a report and there will always be compromises. You never have unlimited resources, so you are trying to balance designing the most informative study with maximizing the resources available. 

::: {.callout-tip}
#### Try this

With decision making in mind, we can tweak the arguments to see how it affects the sample size we need. We will tweak one argument at a time, so your starting point will be the arguments we started with above. 

1. If we used a one-tailed test predicting a positive effect (`"greater"`), we would need `r fitb(60)` participants per group (N = `r fitb(120)`).

2. If we wanted to make fewer type I errors and reduce alpha to .005, we would need `r fitb(117)` participants per group (N = `r fitb(234)`).

3. If we were happy with a larger beta and reduce power to .80 (80%), we would need `r fitb(55)` participants per group (N = `r fitb(110)`).

4. If we wanted to decrease our smallest effect size of interest and set d = .40, we would need `r fitb(133)` participants per group (N = `r fitb(266)`).

5. If we thought it was appropriate to change the design to within-subjects and use a paired-samples t-test instead (`"paired"`), we would need `r fitb(39)` participants.
:::

::: {.callout-caution collapse="true"}
#### Show me the solution

1. We can calculate sample size for a one-tailed test by entering `alternative = "greater"` or `alternative = "less"`. This must match the effect size direction or you will receive an error. 

```{r}
pwr.t.test(n = NULL, 
           d = 0.54, 
           sig.level = .05, 
           power = .90, 
           type = "two.sample", 
           alternative = "greater") %>% 
  pluck("n") %>% 
  ceiling()
```

2. We can decrease alpha by entering `alpha = .005` to calculate the sample size for reducing the type I error rate. 

```{r}
pwr.t.test(n = NULL, 
           d = 0.54, 
           sig.level = .005, 
           power = .90, 
           type = "two.sample", 
           alternative = "two.sided") %>% 
  pluck("n") %>% 
  ceiling()
```

3. We can decrease power if we were happy with a larger beta / type II error rate by entering `power = .80`.

```{r}
pwr.t.test(n = NULL, 
           d = 0.54, 
           sig.level = .05, 
           power = .80, 
           type = "two.sample", 
           alternative = "two.sided") %>% 
  pluck("n") %>% 
  ceiling()
```

4. We can decrease the smallest effect size of interest if we wanted the study to be more sensitive by entering `d = .40`. 

```{r}
pwr.t.test(n = NULL, 
           d = 0.40, 
           sig.level = .05, 
           power = .90, 
           type = "two.sample", 
           alternative = "two.sided") %>% 
  pluck("n") %>% 
  ceiling()
```

5. If we changed the design and test to within-subjects, we can enter `type = "paired"`. 

```{r}
pwr.t.test(n = NULL, 
           d = 0.54, 
           sig.level = .05, 
           power = .90, 
           type = "paired", 
           alternative = "two.sided") %>% 
  pluck("n") %>% 
  ceiling()
```
:::

These are important lessons to recognise which inputs increase and which decrease the sample size you need. For an *a priori* power analysis in the design phase, you can tweak the inputs depending on how sensitive you want your study given the resources available to you. Holding everything else constant, we can summarise the patterns as:

- Using a one-tailed test `r mcq(sample(c("increases", answer = "decreases")))` the sample size you need.

- Reducing alpha `r mcq(sample(c(answer = "increases", "decreases")))` the sample size you need. 

- Reducing power / increasing beta `r mcq(sample(c("increases", answer = "decreases")))` the sample size you need. 

- Reducing the smallest effect size of interest `r mcq(sample(c(answer = "increases", "decreases")))` the sample size you need.

- Switching to a within-subjects design `r mcq(sample(c("increases", answer = "decreases")))` the sample size you need.

### Sensitivity power analysis

Now imagine you already knew the sample size or had access to a population of a known size. In this scenario, you would conduct a sensitivity power analysis. This would tell you what effect sizes your study would be powered to detect for a given alpha, power, and sample size. This is helpful for interpreting your results as you can outline what effect sizes your study was sensitive to and which effects would be too small for you to reliably detect.

Imagine we had finished collecting data and we knew we had 40 participants in each group but did not conduct an *a priori* power analysis when designing the study. Instead of leaving `n` blank, we can leave `d` blank.

```{r}
pwr.t.test(n = 40, 
           d = NULL, 
           sig.level = .05, 
           power = .90, 
           type = "two.sample", 
           alternative = "two.sided")
```

The output tells us that the study is sensitive to detect effect sizes of d = 0.73 with 90% power. This helps us to interpret the results if we did not plan with power in mind. If the effect size we could detect with 90% power is larger than our smallest effect size of interest, our study was potentially underpowered. This might sound like post-hoc power we warned you about, but the key difference is you are comparing the effect size your study was sensitive to against your smallest effect size of interest, **not** your observed effect size.

For a sensitivity power analysis, you will often find yourself with unequal sample sizes. The previous function assumes equal sample sizes, but `pwr.t2n.test()` lets you set `n1` and `n2`. For example, imagine we ended up with two groups of 39 and 43 participants.  

```{r}
pwr.t2n.test(n1 = 39, 
             n2 = 43,
             d = NULL, 
             sig.level = .05, 
             power = .90, 
             alternative = "two.sided") %>% 
  pluck("d") %>% 
  round(digits = 2)
```

```{r echo=FALSE}
power_table <- expand.grid(
  effects = seq(0, 0.9, 0.025),
  sample_size = c(40, 80),
  power = 0)

for (i in 1:nrow(power_table)){
  power <- pwr.t.test(d = power_table$effects[i],
                      sig.level = .05,
                      n = power_table$sample_size[i],
                      type = "two.sample",
                      alternative = "two.sided")$power
  
  power_table$power[i] <- round(power, 2)
}

curve_40 <- power_table %>% 
  filter(sample_size == 40) %>% 
  ggplot(aes(x = effects, y = power)) +
  geom_point() + 
  geom_line() +
  theme_cowplot() + 
  geom_rect(aes(xmin = 0, xmax = .75, ymin = 0, ymax = 0.9), 
            fill="grey",
            alpha=0.01) + 
  geom_vline(xintercept = .75) + 
  geom_hline(yintercept = .90) + 
  scale_x_continuous(name = "Cohen's d", 
                     breaks = seq(0, 0.9, 0.1)) + 
  scale_y_continuous(name = "Statistical Power", 
                     breaks = seq(0, 1, 0.2), 
                     limits = c(0, 1), 
                     labels = scales::percent_format(accuracy = 1)) +
  labs(title = "40 participants per group and 90% power")

curve_80 <- power_table %>% 
  filter(sample_size == 80) %>% 
  ggplot(aes(x = effects, y = power)) +
  geom_point() + 
  geom_line() +
  theme_cowplot() + 
  geom_rect(aes(xmin = 0, xmax = .525, ymin = 0, ymax = 0.9), 
            fill="grey",
            alpha=0.01) + 
  geom_vline(xintercept = .525) + 
  geom_hline(yintercept = .90) + 
  scale_x_continuous(name = "Cohen's d", 
                     breaks = seq(0, 0.9, 0.1)) + 
  scale_y_continuous(name = "Statistical Power", 
                     breaks = seq(0, 1, 0.2), 
                     limits = c(0, 1), 
                     labels = scales::percent_format(accuracy = 1)) + 
  labs(title = "80 participants per group and 90% power")

```

One other key point here is power exists along a curve, there is not just a single value for power once your sample size is fixed. We can visualise this through something called a power curve. @fig-power-curve-40 shows statistical power against Cohen's d as the effect size for a fixed sample of 40 participants per group. We would have 90% power to detect a Cohen's d of 0.75 (the value is a little different here as we set the effect size, rather than calculate it as the output), shown where the two lines meet. You would have more and more power to detect effects larger than 0.75 (follow the curve to the right), but less power to detect effects smaller than 0.75 (follow the curve to the left). The grey shaded region highlights the effects your study would be less sensitive to. 

```{r power-curve-40, echo=FALSE}
#| label: fig-power-curve-40
#| fig.cap: "Power curve for 40 participants per group and 90% power."

curve_40

```

On the other hand, @fig-power-curve-80 shows statistical power against Cohen's d as the effect size for a fixed sample of 80 participants per group. This time, we would have 90% power to detect effects of d = 0.53 and there is a smaller grey region. We would have more power to detect effects larger than d = 0.53 but less power to detect effects smaller than 0.53. 

```{r power-curve-80, echo=FALSE}
#| label: fig-power-curve-80
#| fig.cap: "Power curve for 80 participants per group and 90% power."

curve_80

```

Hopefully, these demonstrations reinforce the idea of sensitivity and how power exists along a curve once you have a fixed sample size.

### Power for regression with a categorical predictor

In Chapters 8 and 9, we recommended expressing your designs as linear regression models. They have many benefits, but one downside is the effect size and process we need for power analysis is not the most intuitive. Typically, people report effect sizes like Cohen's d when comparing groups, but here we need $f^2$. You can convert between effect sizes and we recommend the website [psychometrica.de](https://www.psychometrica.de/effect_size.html#interpretation ){target="_blank"} which has an online calculator for converting effect sizes in section 14. Alternatively, you can use the following code to save $f^2$ as an object. 

```{r}
# enter your Cohen's d value
d <- 0.54

# This calculates f2 from d
f2 <- (d / 2)^2
```

Now we have $f^2$, we can use the function `pwr.f2.test()` which calculates power for regression models. For the equivalent of a t-test, we have the following new arguments: 

- `u`: The numerator degrees of freedom, the number of predictors in your model. 

- `v`: The denominator degrees of freedom, a little more awkward but the sample size minus u minus 1. 

- `f2`: The effect size $f^2$, which is a kind of transformed version of $R^2$ for the amount of variance explained by the model. 

For our power analysis, we will save the inputs as objects to make it easier to reuse them, and enter them in the following arguments: 

```{r}
# number of predictors
u <- 1

# alpha for type I error rate
alpha <- .05

# power for 1-beta
power <-  .90

pwr.f2.test(u = u, 
            v = NULL, 
            sig.level = alpha, 
            power = power, 
            f2 = f2)
```

Using the objects from the power analysis, we can calculate the sample size we need with a little reorganising. 

```{r}
irving_v <- pwr.f2.test(u = 1, 
                        v = NULL, 
                        sig.level = .05, 
                        power = .90, 
                        f2 = f2) %>% 
  pluck("v") 

ceiling(irving_v + u + 1)
```

In the t-test power analysis function, we needed 148 participants in total, so this is off by 1 participant. There are a few steps for rounding errors here, so this is close enough to show it is the equivalent but more awkward process. 

## Power analysis for correlations / continuous predictors

### Introduction to the study

For this section, we need a new study to work with for a correlation / continuous predictor. @wingen_no_2020 were interested in the relationship between the replication rate in psychology studies and the public trust in psychology research. The replication crisis has led to a lot of introspection in the field to consider how we conduct robust research. However, being honest about the state of the field might be good for science, but perhaps it is related to lower public trust in science. We will focus on study 1 which asked the question: does trust in psychology correlate with expected replicability? 

@wingen_no_2020 reported a power analysis and they aimed for 95% power, 5% alpha, and their smallest effect size of interest was *r* = .20. Like Irving et al. (2022), they chose this value based on a meta-analysis which summarised hundreds of studies across social psychology. We will use these values as a starting point and adapt them to see it's impact on the sample size we need.

### A priori power analysis

For Pearson's *r* correlation, there is the function `pwr.r.test()`. All the arguments are the same as for the t-test, apart from we specify *r* as an effect size instead of Cohen's d and we do not need to specify the type of test. 

```{r}
pwr.r.test(n = NULL, 
           r = .20, 
           sig.level = .05, 
           power = .95, 
           alternative = "two.sided")
```

As before, we can isolate the sample size we would need for a study sensitive to these inputs. 

```{r}
pwr.r.test(n = NULL, 
           r = .20, 
           sig.level = .05, 
           power = .95, 
           alternative = "two.sided") %>% 
  pluck("n") %>% 
  ceiling()
```

Our starting point is we need 319 participants to detect our smallest effect size of interest *r* = .20 with 95% power and 5% alpha. 

::: {.callout-tip}
#### Try this

With decision making in mind, we can tweak the arguments to see how it affects the sample size we need. We will tweak one argument at a time, so your starting point will be the arguments we started with above. 

1. If we used a one-tailed test predicting a positive relationship, we would need `r fitb(266)` participants. This reproduces the power analysis from Wingen et al., as they used a one-tailed test. 

2. If we wanted to make fewer type I errors and reduce alpha to .005, we would need `r fitb(485)` participants.

3. If we were happy with a larger beta and reduce power to .80 (80%), we would need `r fitb(194)` participants.
:::

::: {.callout-caution collapse="true"}
#### Show me the solution

1. We can calculate sample size for a one-tailed test by entering `alternative = "greater"` or `alternative = "less"`. This must match the effect size direction or you will receive an error. 

```{r}
pwr.r.test(n = NULL, 
           r = .20, 
           sig.level = .05, 
           power = .95, 
           alternative = "greater") %>% 
  pluck("n") %>% 
  ceiling()
```

2. We can decrease alpha by entering `alpha = .005` to calculate the sample size for reducing the type I error rate. 

```{r}
pwr.r.test(n = NULL, 
           r = .20, 
           sig.level = .005, 
           power = .95, 
           alternative = "two.sided") %>% 
  pluck("n") %>% 
  ceiling()
```

3. We can decrease power if we were happy with a larger beta / type II error rate by entering `power = .80`.

```{r}
pwr.r.test(n = NULL, 
           r = .20, 
           sig.level = .05, 
           power = .80, 
           alternative = "two.sided") %>% 
  pluck("n") %>% 
  ceiling()
```
:::

Like for the independent samples t-test, the design phase allows you to carefully consider the inputs you choose and tweak them depending on how sensitive you want your study given the resources available to you. Holding everything else constant, we can summarise the patterns here as:

- Using a one-tailed test `r mcq(sample(c("increases", answer = "decreases")))` the sample size you need.

- Reducing alpha `r mcq(sample(c(answer = "increases", "decreases")))` the sample size you need. 

- Reducing power / increasing beta `r mcq(sample(c("increases", answer = "decreases")))` the sample size you need. 

### Sensitivity power analysis

@wingen_no_2020 is a great example of a sensitivity power analysis in the wild as they are relatively rare to see in published research. They explain they recruited participants online, so they ended up with more participants than they aimed for. 

::: {.callout-tip}
#### Try this
Their final sample size was **271**, so try and adapt the function to calculate the effect size *r* they were sensitive to. Both of these answers assume 5% alpha and a one-sided test.

To 2 decimal places, they could detect an effect of *r* = `r fitb(0.15)` with 80% power and *r* = `r fitb(0.20, tol = 0.001)` with 95% power. 
:::

::: {.callout-caution collapse="true"}
#### Show me the solution

You should have had this in a code chunk for 80% power: 

```{r}
pwr.r.test(n = 271, 
           r = NULL, 
           sig.level = .05, 
           power = .80, 
           alternative = "greater") %>% 
  pluck("r") %>% 
  round(digits = 2)
```

And this in a code chunk for 95% power:

```{r}
pwr.r.test(n = 271, 
           r = NULL, 
           sig.level = .05, 
           power = .95, 
           alternative = "greater") %>% 
  pluck("r") %>% 
  round(digits = 2)
```
:::

### Power for regression with a continuous predictor

Like the categorical predictor, we have a mismatch between the effect size you typically see reported (Pearson's *r*) and the effect size we need for a regression power analysis ($f^2$). The website [psychometrica.de](https://www.psychometrica.de/effect_size.html#interpretation ){target="_blank"} still works for converting effect sizes in section 14. Alternatively, you can use the following code to save $f^2$ as an object. 

```{r}
# effect size as Pearson's r
r <- .20

# convert to f2 by squaring r values
f2 <- r^2 / (1 - r^2)
```

Now we have $f^2$, we can use the function `pwr.f2.test()` which calculates power for regression models.

```{r}
# number of predictors
u <- 1

# alpha for type I error rate
alpha <- .05

# power for 1-beta
power <-  .95

pwr.f2.test(u = u, 
            v = NULL, 
            sig.level = alpha, 
            power = power, 
            f2 = f2)
```

Using the objects from the power analysis, we can calculate the sample size we need with a little reorganising. 

```{r}
wingen_v <- pwr.f2.test(u = 1, 
                        v = NULL, 
                        sig.level = .05, 
                        power = .95, 
                        f2 = f2) %>% 
  pluck("v") 

ceiling(wingen_v + u + 1)
```

In the Pearson's *r* power analysis function, we needed 319 participants in total, so the estimate is off by 5 participants this time. We still have a few steps for rounding error, so this is close enough to show it is the equivalent but more awkward process. 

## Reporting a power analysis

Bakker et al. (2020) warned that only 20% of power analyses contained enough information to be fully reproducible. To report your power analysis, the reader needs the following four key pieces of information:
- The type of test being conducted,  

- The software used to calculate power,

- The inputs that you used, and 

- Why you chose those inputs. 

For the original example in Figure 3, we could report it like this:

> “To detect an effect size of Cohen’s d = 0.54 with 90% power (alpha = .05, two-tailed), the jpower module in jamovi suggests we would need 74 participants per group (N = 148) for an independent samples t-test. Similar to Irving et al. (2022), the smallest effect size of interest was set to d = 0.54, but we used a two-tailed test as we were less certain about the direction of the effect.” 

This provides the reader with all the information they would need in order to reproduce the power analysis and ensure you have calculated it accurately. The statement also includes your justification for the smallest effect size of interest. Please note there is no single ‘correct’ way to report a power analysis. Just be sure that you have the four key pieces of information. 

## Test Yourself

To end the chapter, we have some knowledge check questions to test your understanding of the concepts we covered in the chapter. Compared to previous chapters, there are no error mode questions as the content is so similar to Chapter 8. We are just going to test your understanding of the concepts rather than potential errors.  

### Knowledge check

1. Assuming you were running a between-subjects t-test on secondary data ($\alpha = .05$, Power = .8, alternative = two-tailed) and that this secondary data has 100 participants in both groups. The smallest effect size, to three decimal places, you could determine with this data is: `r longmcq(sample(c(answer = "d = 0.398", "d = 0.399", "d = 0.280", "d = 0.281")))`

`r hide()`
The code for this test would be:

```{r, eval = FALSE}
pwr.t.test(n = 100, 
           sig.level = .05, 
           power = .8,
           type = "two.sample",
           alternative = "two.sided") %>% 
  pluck("d") %>% 
  round(3)
```

* Meaning that the smallest effect size would be d = 0.39

`r unhide()`

2. Assuming you were running a between-subjects t-test on secondary data ($\alpha = .05$, Power = .8, alternative = two-tailed) and that this secondary data has 60 participants in Group 1 and 40 participants in Group 2. The smallest effect size, to three decimal places, you could determine with this data is: `r longmcq(sample(c(answer = "d = 0.578", "d = 0.577", "r = .578", "r = .577")))`

`r hide()`
The code for this test would be:

```{r, eval = FALSE}
pwr.t2n.test(n1 = 60,
           n2 = 40,
           sig.level = .05, 
           power = .8,
           alternative = "two.sided") %>% 
  pluck("d") %>% 
  round(3)
```

* Meaning that the smallest effect size would be d = 0.578

`r unhide()`

3. Assuming you ran a correlation on secondary data ($\alpha = .05$, Power = .8, alternative = two-tailed) and that this secondary data has 50 observations. The smallest effect size, to three decimal places, you could determine with this data is: `r longmcq(sample(c(answer = "r = .384", "r = .385", "r = .275", "r = .276")))`

`r hide()`
The code for this test would be:

```{r, eval = FALSE}
pwr.r.test(n = 50,
           sig.level = .05, 
           power = .8,
           alternative = "two.sided") %>% 
  pluck("r") %>% 
  round(3)
```

* Meaning that the smallest effect size would be r = .384

`r unhide()`

### Error mode

The following questions are designed to introduce you to making and fixing errors. For this topic, we focus on simple linear regression between two continuous variables. There are not many outright errors that people make here, more misspecifications that are not doing what you think they are doing. 

- Mismatch between effect size and tails

## Words from this Chapter

Below you will find a list of words that were used in this chapter that might be new to you in case it helps to have somewhere to refer back to what they mean. The links in this table take you to the entry for the words in the [PsyTeachR Glossary](https://psyteachr.github.io/glossary/){target="_blank"}. Note that the Glossary is written by numerous members of the team and as such may use slightly different terminology from that shown in the chapter.

```{r gloss, echo=FALSE, results='asis'}
glossary_table()
```

## End of Chapter

**Great!** Hopefully you are now starting to see the interaction between alpha, power, effect sizes, and sample size. We should always want really high powered studies and depending on the size of the effect we are interested in (small to large), and our $\alpha$ level, this will mean we will need to run more or less participants to make sure our study is well powered. Points to note:

* Lowering the $\alpha$ level (e.g. .05 to .01) will reduce the power.
* Lowering the effect size (e.g. .8 to .2) will reduce the power.
* Increasing power (.8 to .9) will require more participants.

A high-powered study looking to detect a small effect size at a low alpha will require a large number of participants!

There are additional functions in the `pwr` package for other types of statistical analyses. We will include these calculates as part of the ANOVA and regression chapters. 

If you want more examples of power to reinforce your understanding, go back and calculate the power of the t-tests, correlations, and chi-squares from earlier chapters. 
